{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd74f74",
   "metadata": {},
   "source": [
    "# Satellite Interference Analysis for RFI Pipeline Data\n",
    "\n",
    "This notebook analyzes RFI pipeline output files and correlates them with Starlink satellite positions to identify potential interference sources. We'll focus on observations in the 10.6-11.2 GHz range and check for satellites within 1 degree of the observation beam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25b9a6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bf1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Working directory: /mnt_home/andresl/rfi-pipeline/experiments\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "import requests\n",
    "from io import StringIO\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Skyfield for satellite tracking\n",
    "from skyfield.api import load, Topos, utc\n",
    "from skyfield.timelib import Time\n",
    "\n",
    "# For angular separation calculations\n",
    "from skyfield.units import Angle\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7f052",
   "metadata": {},
   "source": [
    "## 2. Load and Parse RFI Pipeline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8a4c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /datax/scratch/andresl/pipeline-runs/all-bands/full-run-60k/files.csv\n",
      "Loaded 60795 files\n",
      "Columns: ['file', 'num_hits', 'time_completed', 'RA', 'DEC', 'tstart', 'nchans', 'foff', 'fch1', 'flch']\n",
      "\n",
      "First few rows:\n",
      "                                                file  num_hits  \\\n",
      "0  /datag/pipeline/AGBT24B_999_21/blc04_blp04/blc...         0   \n",
      "1  /datag/pipeline/AGBT22B_999_41/blc02_blp02/blc...         0   \n",
      "2  /datag/pipeline/AGBT24A_999_07/blc06_blp06/blc...        87   \n",
      "3  /datag/pipeline/AGBT24A_999_20/blc20_blp10/blc...        34   \n",
      "4  /datag/pipeline/AGBT23B_999_38/blc61_blp21/blc...         8   \n",
      "\n",
      "                     time_completed         RA      DEC        tstart  \\\n",
      "0  2025-08-03T06:49:30.027120+00:00   4.035467 -10.9115  60652.104861   \n",
      "1  2025-08-03T06:49:30.755366+00:00  12.424133  13.1221  60054.954826   \n",
      "2  2025-08-03T06:49:44.751474+00:00  19.220073  76.1255  60384.226481   \n",
      "3  2025-08-03T06:49:48.961125+00:00  22.853027  48.7344  60415.838333   \n",
      "4  2025-08-03T06:49:49.295092+00:00   0.940100 -14.7890  60349.013322   \n",
      "\n",
      "       nchans      foff          fch1          flch  \n",
      "0  67108864.0 -0.000003  11626.464844  11438.964844  \n",
      "1  67108864.0 -0.000003  12001.464844  11813.964844  \n",
      "2  67108864.0 -0.000003   1126.464844    938.964844  \n",
      "3  67108864.0 -0.000003  11251.464844  11063.964844  \n",
      "4  67108864.0 -0.000003   9938.964844   9751.464844  \n",
      "\n",
      "Frequency range summary:\n",
      "Min frequency: 563.96 MHz\n",
      "Max frequency: 12376.46 MHz\n",
      "\n",
      "Frequency range summary:\n",
      "Min frequency: 563.96 MHz\n",
      "Max frequency: 12376.46 MHz\n"
     ]
    }
   ],
   "source": [
    "# Path to the files.csv from RFI pipeline run\n",
    "files_csv_path = \"/datax/scratch/andresl/pipeline-runs/all-bands/full-run-60k/files.csv\"\n",
    "\n",
    "print(f\"Loading data from: {files_csv_path}\")\n",
    "\n",
    "# Load the files.csv\n",
    "df = pd.read_csv(files_csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} files\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Parse timestamps and convert to datetime objects\n",
    "df['time_completed_dt'] = pd.to_datetime(df['time_completed'])\n",
    "\n",
    "# Calculate frequency range for each file (in MHz)\n",
    "# fch1 is the frequency of the first channel, foff is the frequency step\n",
    "# flch is the frequency of the last channel (fch1 + foff * nchans)\n",
    "df['freq_start_mhz'] = df['flch']  # Lower frequency bound\n",
    "df['freq_end_mhz'] = df['fch1']    # Upper frequency bound\n",
    "\n",
    "print(f\"\\nFrequency range summary:\")\n",
    "print(f\"Min frequency: {df['freq_start_mhz'].min():.2f} MHz\")\n",
    "print(f\"Max frequency: {df['freq_end_mhz'].max():.2f} MHz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466136a",
   "metadata": {},
   "source": [
    "## 3. Filter Files by Frequency Range\n",
    "\n",
    "We want to find files with significant overlap with the 10.6-11.2 GHz range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f25f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target frequency range: 10600-11200 MHz\n",
      "\n",
      "Found 255 files with ‚â•50% overlap\n",
      "Overlap percentage range: 66.9% - 100.0%\n",
      "\n",
      "Top files by overlap percentage:\n",
      "  spliced_blc00010203040506o7o0111213141516o7o031323334353637_guppi_58328_47198_HIP3909_0036.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58417_13701_HIP112870_0013.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58513_06113_HIP26228_0058.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58223_01045_HIP26686_0066.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58511_42568_HIP56242_0034.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "\n",
      "Found 255 files with ‚â•50% overlap\n",
      "Overlap percentage range: 66.9% - 100.0%\n",
      "\n",
      "Top files by overlap percentage:\n",
      "  spliced_blc00010203040506o7o0111213141516o7o031323334353637_guppi_58328_47198_HIP3909_0036.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58417_13701_HIP112870_0013.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58513_06113_HIP26228_0058.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58223_01045_HIP26686_0066.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n",
      "  spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58511_42568_HIP56242_0034.gpuspec.0000.h5: 7501.5-11251.5 MHz (100.0% overlap)\n"
     ]
    }
   ],
   "source": [
    "# Target frequency range (in MHz)\n",
    "target_freq_min = 10600  # 10.6 GHz\n",
    "target_freq_max = 11200  # 11.2 GHz\n",
    "\n",
    "print(f\"Target frequency range: {target_freq_min}-{target_freq_max} MHz\")\n",
    "\n",
    "# Function to calculate overlap between two frequency ranges\n",
    "def calculate_overlap(start1, end1, start2, end2):\n",
    "    \"\"\"Calculate the overlap between two frequency ranges\"\"\"\n",
    "    overlap_start = max(start1, start2)\n",
    "    overlap_end = min(end1, end2)\n",
    "    overlap = max(0, overlap_end - overlap_start)\n",
    "    return overlap\n",
    "\n",
    "# Calculate overlap for each file\n",
    "df['overlap_mhz'] = df.apply(\n",
    "    lambda row: calculate_overlap(\n",
    "        row['freq_start_mhz'], row['freq_end_mhz'],\n",
    "        target_freq_min, target_freq_max\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Calculate overlap percentage (relative to target range)\n",
    "target_range_width = target_freq_max - target_freq_min\n",
    "df['overlap_percentage'] = (df['overlap_mhz'] / target_range_width) * 100\n",
    "\n",
    "# Filter files with significant overlap (e.g., >50% overlap)\n",
    "min_overlap_percentage = 50\n",
    "filtered_df = df[df['overlap_percentage'] >= min_overlap_percentage].copy()\n",
    "\n",
    "print(f\"\\nFound {len(filtered_df)} files with ‚â•{min_overlap_percentage}% overlap\")\n",
    "print(f\"Overlap percentage range: {filtered_df['overlap_percentage'].min():.1f}% - {filtered_df['overlap_percentage'].max():.1f}%\")\n",
    "\n",
    "if len(filtered_df) > 0:\n",
    "    print(f\"\\nTop files by overlap percentage:\")\n",
    "    top_overlap = filtered_df.nlargest(5, 'overlap_percentage')[['file', 'freq_start_mhz', 'freq_end_mhz', 'overlap_percentage']]\n",
    "    for idx, row in top_overlap.iterrows():\n",
    "        filename = Path(row['file']).name\n",
    "        print(f\"  {filename}: {row['freq_start_mhz']:.1f}-{row['freq_end_mhz']:.1f} MHz ({row['overlap_percentage']:.1f}% overlap)\")\n",
    "else:\n",
    "    print(\"No files found with sufficient overlap. Reducing threshold...\")\n",
    "    min_overlap_percentage = 10\n",
    "    filtered_df = df[df['overlap_percentage'] >= min_overlap_percentage].copy()\n",
    "    print(f\"Found {len(filtered_df)} files with ‚â•{min_overlap_percentage}% overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8370f4",
   "metadata": {},
   "source": [
    "## 4. Sample Random Files\n",
    "\n",
    "Randomly select 3 files from the filtered dataset for detailed satellite analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e6b2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 files for satellite analysis:\n",
      "================================================================================\n",
      "File: spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58247_35418_HIP88601_0053.gpuspec.0000.h5\n",
      "  RA/DEC: 18.091¬∞, 2.496¬∞\n",
      "  Start time (MJD): 58247.409931\n",
      "  Frequency: 7501.5-11251.5 MHz\n",
      "  Target overlap: 100.0%\n",
      "\n",
      "File: spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58511_42568_HIP56242_0034.gpuspec.0000.h5\n",
      "  RA/DEC: 11.529¬∞, 14.362¬∞\n",
      "  Start time (MJD): 58511.492685\n",
      "  Frequency: 7501.5-11251.5 MHz\n",
      "  Target overlap: 100.0%\n",
      "\n",
      "File: spliced_blc00010203040506o7o0111213141516o7o021222324252627_guppi_58445_82374_HIP112870_0104.gpuspec.0000.h5\n",
      "  RA/DEC: 22.857¬∞, 13.971¬∞\n",
      "  Start time (MJD): 58445.953403\n",
      "  Frequency: 7797.4-11102.1 MHz\n",
      "  Target overlap: 83.7%\n",
      "\n",
      "Total selected files: 3\n"
     ]
    }
   ],
   "source": [
    "# Number of files to sample\n",
    "n_sample = 3\n",
    "\n",
    "if len(filtered_df) >= n_sample:\n",
    "    # Randomly sample files\n",
    "    sample_df = filtered_df.sample(n=n_sample, random_state=42).copy()\n",
    "else:\n",
    "    # If we don't have enough files, take all of them\n",
    "    sample_df = filtered_df.copy()\n",
    "    print(f\"Warning: Only {len(filtered_df)} files available, using all of them\")\n",
    "\n",
    "print(f\"Selected {len(sample_df)} files for satellite analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "selected_files = []\n",
    "for idx, row in sample_df.iterrows():\n",
    "    file_info = {\n",
    "        'file_path': row['file'],\n",
    "        'filename': Path(row['file']).name,\n",
    "        'ra_deg': row['RA'],\n",
    "        'dec_deg': row['DEC'],\n",
    "        'tstart_mjd': row['tstart'],\n",
    "        'freq_start_mhz': row['freq_start_mhz'],\n",
    "        'freq_end_mhz': row['freq_end_mhz'],\n",
    "        'overlap_percentage': row['overlap_percentage'],\n",
    "        'time_completed': row['time_completed_dt']\n",
    "    }\n",
    "    selected_files.append(file_info)\n",
    "    \n",
    "    print(f\"File: {file_info['filename']}\")\n",
    "    print(f\"  RA/DEC: {file_info['ra_deg']:.3f}¬∞, {file_info['dec_deg']:.3f}¬∞\")\n",
    "    print(f\"  Start time (MJD): {file_info['tstart_mjd']:.6f}\")\n",
    "    print(f\"  Frequency: {file_info['freq_start_mhz']:.1f}-{file_info['freq_end_mhz']:.1f} MHz\")\n",
    "    print(f\"  Target overlap: {file_info['overlap_percentage']:.1f}%\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total selected files: {len(selected_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfe470",
   "metadata": {},
   "source": [
    "## 5. Load Satellite Data\n",
    "\n",
    "Load Starlink satellite orbital data using Skyfield. We'll use current TLE data from CelesTrak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Skyfield loader\n",
    "ts = load.timescale()\n",
    "\n",
    "# Check the date range of our observations first\n",
    "print(\"Checking observation date range...\")\n",
    "obs_mjds = [file_info['tstart_mjd'] for file_info in selected_files]\n",
    "obs_dates = [datetime(1858, 11, 17) + timedelta(days=mjd) for mjd in obs_mjds]\n",
    "min_date, max_date = min(obs_dates), max(obs_dates)\n",
    "print(f\"Observation date range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# IMPORTANT: Starlink constellation began in 2019\n",
    "starlink_start_date = datetime(2019, 5, 23)\n",
    "if min_date < starlink_start_date:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some observations are from before {starlink_start_date.strftime('%Y-%m-%d')} when Starlink didn't exist!\")\n",
    "\n",
    "def estimate_starlink_count_by_date(target_date):\n",
    "    \"\"\"Estimate number of operational Starlink satellites by date\"\"\"\n",
    "    if target_date < starlink_start_date:\n",
    "        return 0\n",
    "    \n",
    "    # Rough timeline of Starlink constellation growth\n",
    "    launches = [\n",
    "        (datetime(2019, 5, 24), 60),    # First launch\n",
    "        (datetime(2019, 11, 11), 120),  # Second launch  \n",
    "        (datetime(2020, 1, 7), 180),    # Third launch\n",
    "        (datetime(2020, 6, 1), 500),    # Multiple launches\n",
    "        (datetime(2021, 1, 1), 1000),   # Rapid expansion\n",
    "        (datetime(2022, 1, 1), 2000),   # Continued growth\n",
    "        (datetime(2023, 1, 1), 3500),   # Major expansion\n",
    "        (datetime(2024, 1, 1), 5000),   # Current scale\n",
    "        (datetime(2025, 1, 1), 6000),   # Recent numbers\n",
    "    ]\n",
    "    \n",
    "    for date, count in launches:\n",
    "        if target_date < date:\n",
    "            return count\n",
    "    return 6000  # Current estimate\n",
    "\n",
    "def get_historical_starlink_tles(target_date):\n",
    "    \"\"\"\n",
    "    Fetch Starlink TLE data appropriate for the target date.\n",
    "    Uses multiple strategies for historical data.\n",
    "    \"\"\"\n",
    "    print(f\"  Fetching TLEs for {target_date.strftime('%Y-%m-%d')}...\")\n",
    "    \n",
    "    # If observation is before Starlink existed, return empty\n",
    "    if target_date < starlink_start_date:\n",
    "        print(f\"    No Starlink satellites existed on {target_date.strftime('%Y-%m-%d')}\")\n",
    "        return []\n",
    "    \n",
    "    expected_count = estimate_starlink_count_by_date(target_date)\n",
    "    print(f\"    Expected ~{expected_count} Starlink satellites for this date\")\n",
    "    \n",
    "    # Strategy 1: Try current TLE data (best we can do without credentials)\n",
    "    # In production, you would use Space-Track.org with historical queries\n",
    "    urls_to_try = [\n",
    "        'https://celestrak.org/NORAD/elements/gp.php?GROUP=starlink&FORMAT=tle',\n",
    "    ]\n",
    "    \n",
    "    for url in urls_to_try:\n",
    "        try:\n",
    "            print(f\"    Fetching from: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse TLE data\n",
    "            tle_lines = response.text.strip().split('\\n')\n",
    "            satellites = []\n",
    "            \n",
    "            # Group TLE lines into sets of 3 (name, line1, line2)\n",
    "            for i in range(0, len(tle_lines), 3):\n",
    "                if i + 2 < len(tle_lines):\n",
    "                    name = tle_lines[i].strip()\n",
    "                    line1 = tle_lines[i + 1].strip()\n",
    "                    line2 = tle_lines[i + 2].strip()\n",
    "                    \n",
    "                    # Create satellite object\n",
    "                    if line1.startswith('1 ') and line2.startswith('2 '):\n",
    "                        try:\n",
    "                            from skyfield.sgp4lib import EarthSatellite\n",
    "                            sat = EarthSatellite(line1, line2, name, ts)\n",
    "                            \n",
    "                            # For historical dates, we would ideally filter to only\n",
    "                            # include satellites that existed at that time\n",
    "                            # For now, we'll use all current satellites with a warning\n",
    "                            satellites.append(sat)\n",
    "                        except Exception as e:\n",
    "                            continue  # Skip invalid TLEs\n",
    "            \n",
    "            if satellites:\n",
    "                actual_count = len(satellites)\n",
    "                print(f\"    Loaded {actual_count} satellites (current TLE data)\")\n",
    "                \n",
    "                if target_date < datetime(2024, 1, 1):\n",
    "                    # For historical dates, subsample to approximate constellation size\n",
    "                    if actual_count > expected_count and expected_count > 0:\n",
    "                        # Randomly subsample to approximate historical constellation size\n",
    "                        import random\n",
    "                        random.seed(int(target_date.timestamp()))  # Deterministic sampling\n",
    "                        satellites = random.sample(satellites, min(expected_count, actual_count))\n",
    "                        print(f\"    Subsampled to {len(satellites)} satellites (estimated for {target_date.strftime('%Y-%m-%d')})\")\n",
    "                \n",
    "                return satellites\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"    Could not fetch TLEs for {target_date.strftime('%Y-%m-%d')}\")\n",
    "    return []\n",
    "\n",
    "# Group observations by date to minimize TLE downloads\n",
    "print(f\"\\nüì° Loading Historical TLE Data\")\n",
    "print(\"=\" * 50)\n",
    "obs_by_date = {}\n",
    "for file_info in selected_files:\n",
    "    obs_date = datetime(1858, 11, 17) + timedelta(days=file_info['tstart_mjd'])\n",
    "    date_key = obs_date.strftime('%Y-%m-%d')\n",
    "    if date_key not in obs_by_date:\n",
    "        obs_by_date[date_key] = []\n",
    "    obs_by_date[date_key].append(file_info)\n",
    "\n",
    "print(f\"Found observations on {len(obs_by_date)} different dates:\")\n",
    "for date_str, files in obs_by_date.items():\n",
    "    print(f\"  {date_str}: {len(files)} observations\")\n",
    "\n",
    "# Load TLEs for each unique date\n",
    "starlink_by_date = {}\n",
    "for date_str, files_on_date in obs_by_date.items():\n",
    "    obs_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    print(f\"\\nProcessing {date_str}...\")\n",
    "    \n",
    "    starlink_sats = get_historical_starlink_tles(obs_date)\n",
    "    if starlink_sats:\n",
    "        starlink_by_date[date_str] = {sat.name: sat for sat in starlink_sats}\n",
    "        print(f\"  ‚úÖ Cached {len(starlink_sats)} satellites for {date_str}\")\n",
    "    else:\n",
    "        starlink_by_date[date_str] = {}\n",
    "        print(f\"  ‚ùå No satellites available for {date_str}\")\n",
    "\n",
    "# Summary\n",
    "total_unique_sats = set()\n",
    "for date_sats in starlink_by_date.values():\n",
    "    total_unique_sats.update(date_sats.keys())\n",
    "\n",
    "print(f\"\\nüéØ TLE Loading Summary\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Dates processed: {len(obs_by_date)}\")\n",
    "print(f\"Dates with satellite data: {len([d for d, sats in starlink_by_date.items() if sats])}\")\n",
    "print(f\"Total unique satellites: {len(total_unique_sats)}\")\n",
    "\n",
    "# Show constellation evolution\n",
    "if len(starlink_by_date) > 1:\n",
    "    print(f\"\\nConstellation Evolution:\")\n",
    "    for date_str in sorted(starlink_by_date.keys()):\n",
    "        sat_count = len(starlink_by_date[date_str])\n",
    "        obs_count = len(obs_by_date[date_str])\n",
    "        print(f\"  {date_str}: {sat_count:4d} satellites, {obs_count} observations\")\n",
    "\n",
    "if len(total_unique_sats) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No satellite data loaded. Creating demonstration satellites...\")\n",
    "    from skyfield.sgp4lib import EarthSatellite\n",
    "    \n",
    "    # Create multiple demo satellites for different dates\n",
    "    demo_satellites = []\n",
    "    for i in range(3):\n",
    "        line1 = f\"1 4471{i}U 19074A   21200.12345678  .00001234  00000-0  12345-4 0  9999\"\n",
    "        line2 = f\"2 4471{i}  53.0538 {123.4567 + i*10:.4f} 0001234  {12.3456 + i*5:.4f} 347.6543 15.05123456123456\"\n",
    "        sat = EarthSatellite(line1, line2, f\"STARLINK-DEMO-{i+1}\", ts)\n",
    "        demo_satellites.append(sat)\n",
    "    \n",
    "    # Add demo satellites to all dates that need them\n",
    "    for date_str in starlink_by_date.keys():\n",
    "        if not starlink_by_date[date_str]:\n",
    "            starlink_by_date[date_str] = {sat.name: sat for sat in demo_satellites}\n",
    "    \n",
    "    print(f\"   Added {len(demo_satellites)} demonstration satellites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter observations to only include those from when Starlink existed\n",
    "starlink_start_mjd = 58622  # May 23, 2019 (first Starlink launch)\n",
    "\n",
    "print(\"Filtering observations to Starlink era...\")\n",
    "print(f\"Starlink first launch: MJD {starlink_start_mjd} (May 23, 2019)\")\n",
    "\n",
    "# Filter selected files to only include post-Starlink observations\n",
    "original_count = len(selected_files)\n",
    "selected_files_filtered = [\n",
    "    file_info for file_info in selected_files \n",
    "    if file_info['tstart_mjd'] >= starlink_start_mjd\n",
    "]\n",
    "\n",
    "print(f\"\\nObservation filtering results:\")\n",
    "print(f\"  Original files: {original_count}\")\n",
    "print(f\"  Files from Starlink era: {len(selected_files_filtered)}\")\n",
    "print(f\"  Files filtered out: {original_count - len(selected_files_filtered)}\")\n",
    "\n",
    "if len(selected_files_filtered) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No observations from the Starlink era found in sample!\")\n",
    "    print(\"   Consider selecting different files or expanding the sample size.\")\n",
    "    print(\"   For demonstration, we'll proceed with all files but note the limitations.\")\n",
    "    selected_files_filtered = selected_files\n",
    "    \n",
    "    print(\"\\nüìù Analysis Notes:\")\n",
    "    print(\"   - Pre-2019 observations: No Starlink satellites existed\")\n",
    "    print(\"   - 2019-2021 observations: Fewer Starlink satellites\")\n",
    "    print(\"   - 2022+ observations: Full Starlink constellation\")\n",
    "else:\n",
    "    print(f\"\\nProceeding with {len(selected_files_filtered)} files from the Starlink era.\")\n",
    "\n",
    "# Update our working list\n",
    "selected_files = selected_files_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbac4f",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important Note on Historical TLE Data\n",
    "\n",
    "**Current Limitation:** This notebook uses current TLE (Two-Line Element) data to predict satellite positions for historical observations. This approach has significant limitations:\n",
    "\n",
    "1. **Temporal Accuracy**: Satellite orbits decay and change over time. Using 2025 TLE data to predict 2020 positions can introduce errors of several degrees.\n",
    "\n",
    "2. **Constellation Evolution**: The Starlink constellation has grown from 60 satellites in 2019 to thousands today. Historical analysis should account for the actual number of satellites operational at each observation time.\n",
    "\n",
    "3. **Orbital Maneuvers**: Satellites perform orbit-raising and maintenance maneuvers that aren't captured in current TLEs.\n",
    "\n",
    "**For Production Analysis, Consider:**\n",
    "\n",
    "- **Historical TLE Archives**: Use services like Space-Track.org or CelesTrak archives to get TLE data from the observation epoch\n",
    "- **Constellation Timeline**: Track when each satellite was launched and became operational\n",
    "- **TLE Epoch Matching**: Use TLE data within days/weeks of the observation time\n",
    "- **Multiple Constellations**: Include other satellite systems (GPS, communication satellites) that existed throughout the observation period\n",
    "\n",
    "**This notebook provides a framework that can be extended with proper historical TLE data for more accurate analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1a8bf",
   "metadata": {},
   "source": [
    "## 6. Calculate Satellite Positions\n",
    "\n",
    "For each selected observation file, calculate the positions of all Starlink satellites at the observation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63313fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import EarthLocation\n",
    "import astropy.units as u\n",
    "\n",
    "gbt_loc = EarthLocation.of_site('Green Bank Telescope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed558c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green Bank Observatory coordinates (from the telescope metadata)\n",
    "# These coordinates are approximate for the GBT\n",
    "# gbt_lat = 38.4331  # degrees North\n",
    "# gbt_lon = -79.8397  # degrees West  \n",
    "# gbt_elevation = 824  # meters\n",
    "gbt_lat = gbt_loc.lat.to_value(u.deg)\n",
    "gbt_lon = gbt_loc.lon.to_value(u.deg)\n",
    "gbt_elevation = gbt_loc.height.to_value(u.m)\n",
    "\n",
    "# Create observatory location\n",
    "observatory = Topos(latitude_degrees=gbt_lat, \n",
    "                   longitude_degrees=gbt_lon, \n",
    "                   elevation_m=gbt_elevation)\n",
    "\n",
    "print(f\"Observatory location: {gbt_lat:.4f}¬∞N, {gbt_lon:.4f}¬∞W, {gbt_elevation}m\")\n",
    "\n",
    "def mjd_to_skyfield_time(mjd):\n",
    "    \"\"\"Convert Modified Julian Date to Skyfield Time object\"\"\"\n",
    "    # MJD = JD - 2400000.5\n",
    "    jd = mjd + 2400000.5\n",
    "    return ts.tt_jd(jd)\n",
    "\n",
    "\n",
    "def calculate_angular_separation(ra1_deg, dec1_deg, ra2_deg, dec2_deg):\n",
    "    \"\"\"Calculate angular separation between two sky positions in degrees\"\"\"\n",
    "    # Convert to radians\n",
    "    ra1, dec1 = np.radians(ra1_deg), np.radians(dec1_deg)\n",
    "    ra2, dec2 = np.radians(ra2_deg), np.radians(dec2_deg)\n",
    "    \n",
    "    # Haversine formula for angular separation\n",
    "    dra = ra2 - ra1\n",
    "    ddec = dec2 - dec1\n",
    "    \n",
    "    a = (np.sin(ddec/2)**2 + \n",
    "         np.cos(dec1) * np.cos(dec2) * np.sin(dra/2)**2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return np.degrees(c)\n",
    "\n",
    "print(\"Functions defined for satellite position calculations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c215583",
   "metadata": {},
   "source": [
    "## 7. Find Nearby Satellites\n",
    "\n",
    "Calculate angular separations between each satellite and the observation beam center, and identify satellites within 1 degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc53e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum separation to consider (in degrees)\n",
    "max_separation_deg = 1.0\n",
    "\n",
    "satellite_crossings = []\n",
    "\n",
    "print(f\"Analyzing satellite positions for {len(selected_files)} observation files...\")\n",
    "print(f\"Looking for satellites within {max_separation_deg}¬∞ of beam center\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, file_info in enumerate(selected_files):\n",
    "    print(f\"\\nFile {i+1}/{len(selected_files)}: {file_info['filename']}\")\n",
    "    print(f\"RA/DEC: {file_info['ra_deg']:.3f}¬∞, {file_info['dec_deg']:.3f}¬∞\")\n",
    "    print(f\"Observation time (MJD): {file_info['tstart_mjd']:.6f}\")\n",
    "    \n",
    "    # Get the observation date for TLE lookup\n",
    "    obs_date = datetime(1858, 11, 17) + timedelta(days=file_info['tstart_mjd'])\n",
    "    date_key = obs_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Observation date: {date_key}\")\n",
    "    \n",
    "    # Get satellite data for this observation date\n",
    "    starlink_dict = starlink_by_date.get(date_key, {})\n",
    "    if not starlink_dict:\n",
    "        print(f\"  No satellite data available for {date_key}\")\n",
    "        satellite_crossings.append({\n",
    "            'file_info': file_info,\n",
    "            'nearby_satellites': [],\n",
    "            'num_nearby_satellites': 0,\n",
    "            'min_separation_deg': None,\n",
    "            'closest_satellite': None\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Using {len(starlink_dict)} satellites from {date_key} TLE data\")\n",
    "    \n",
    "    # Convert observation time to Skyfield format\n",
    "    obs_time = mjd_to_skyfield_time(file_info['tstart_mjd'])\n",
    "    \n",
    "    nearby_satellites = []\n",
    "    min_separation = float('inf')\n",
    "    closest_satellite = None\n",
    "    \n",
    "    # Check each Starlink satellite for this date\n",
    "    satellites_checked = 0\n",
    "    satellites_computed = 0\n",
    "    \n",
    "    for sat_name, satellite in starlink_dict.items():\n",
    "        satellites_checked += 1\n",
    "        try:\n",
    "            # Get satellite position at observation time as seen from the observatory\n",
    "            geometry = satellite.at(obs_time)\n",
    "            topocentric = geometry - observatory.at(obs_time)\n",
    "            \n",
    "            # Get RA/Dec coordinates\n",
    "            ra, dec, distance = topocentric.radec()\n",
    "            satellites_computed += 1\n",
    "            \n",
    "            # Calculate angular separation from observation beam center\n",
    "            separation = calculate_angular_separation(\n",
    "                file_info['ra_deg'], file_info['dec_deg'],\n",
    "                ra.degrees, dec.degrees\n",
    "            )\n",
    "            \n",
    "            # Check if within threshold\n",
    "            if separation <= max_separation_deg:\n",
    "                sat_info = {\n",
    "                    'satellite_name': sat_name,\n",
    "                    'separation_deg': separation,\n",
    "                    'sat_ra_deg': ra.degrees,\n",
    "                    'sat_dec_deg': dec.degrees,\n",
    "                    'sat_distance_km': distance.km,\n",
    "                    'observation_date': date_key\n",
    "                }\n",
    "                nearby_satellites.append(sat_info)\n",
    "                \n",
    "                # Track closest satellite\n",
    "                if separation < min_separation:\n",
    "                    min_separation = separation\n",
    "                    closest_satellite = sat_name\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Skip satellites that can't be computed (e.g., due to epoch issues)\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Checked {satellites_checked} satellites, computed {satellites_computed} positions\")\n",
    "    \n",
    "    # Store results for this file\n",
    "    crossing_info = {\n",
    "        'file_info': file_info,\n",
    "        'nearby_satellites': nearby_satellites,\n",
    "        'num_nearby_satellites': len(nearby_satellites),\n",
    "        'min_separation_deg': min_separation if min_separation != float('inf') else None,\n",
    "        'closest_satellite': closest_satellite,\n",
    "        'observation_date': date_key,\n",
    "        'satellites_available': len(starlink_dict)\n",
    "    }\n",
    "    satellite_crossings.append(crossing_info)\n",
    "    \n",
    "    # Print results for this file\n",
    "    if len(nearby_satellites) > 0:\n",
    "        print(f\"  Found {len(nearby_satellites)} satellites within {max_separation_deg}¬∞:\")\n",
    "        print(f\"    Closest: {closest_satellite} at {min_separation:.3f}¬∞\")\n",
    "        \n",
    "        # Show up to 5 closest satellites\n",
    "        sorted_sats = sorted(nearby_satellites, key=lambda x: x['separation_deg'])\n",
    "        for j, sat in enumerate(sorted_sats[:5]):\n",
    "            print(f\"      {sat['satellite_name']}: {sat['separation_deg']:.3f}¬∞ separation\")\n",
    "        if len(sorted_sats) > 5:\n",
    "            print(f\"      ... and {len(sorted_sats) - 5} more\")\n",
    "    else:\n",
    "        print(f\"  No satellites found within {max_separation_deg}¬∞\")\n",
    "\n",
    "print(f\"\\nCompleted satellite analysis for all {len(selected_files)} files.\")\n",
    "\n",
    "# Summary of TLE usage\n",
    "tle_dates_used = set(crossing['observation_date'] for crossing in satellite_crossings)\n",
    "print(f\"\\nTLE Data Summary:\")\n",
    "print(f\"  Unique observation dates: {len(tle_dates_used)}\")\n",
    "for date in sorted(tle_dates_used):\n",
    "    num_sats = len(starlink_by_date.get(date, {}))\n",
    "    files_on_date = len([c for c in satellite_crossings if c['observation_date'] == date])\n",
    "    print(f\"    {date}: {num_sats} satellites, {files_on_date} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf733920",
   "metadata": {},
   "source": [
    "## 8. Analyze Results\n",
    "\n",
    "Create a summary of satellite crossings and visualize the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcccb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "print(\"SATELLITE CROSSING ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_files = len(satellite_crossings)\n",
    "files_with_satellites = sum(1 for crossing in satellite_crossings if crossing['num_nearby_satellites'] > 0)\n",
    "total_satellite_detections = sum(crossing['num_nearby_satellites'] for crossing in satellite_crossings)\n",
    "\n",
    "print(f\"Total files analyzed: {total_files}\")\n",
    "print(f\"Files with nearby satellites: {files_with_satellites}\")\n",
    "print(f\"Percentage with satellites: {(files_with_satellites/total_files)*100:.1f}%\")\n",
    "print(f\"Total satellite detections: {total_satellite_detections}\")\n",
    "\n",
    "if files_with_satellites > 0:\n",
    "    avg_satellites_per_file = total_satellite_detections / files_with_satellites\n",
    "    print(f\"Average satellites per file (with detections): {avg_satellites_per_file:.1f}\")\n",
    "\n",
    "print(\"\\nDETAILED RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "for i, crossing in enumerate(satellite_crossings):\n",
    "    file_info = crossing['file_info']\n",
    "    print(f\"\\nFile {i+1}: {file_info['filename']}\")\n",
    "    print(f\"  Position: RA={file_info['ra_deg']:.3f}¬∞, DEC={file_info['dec_deg']:.3f}¬∞\")\n",
    "    print(f\"  Frequency overlap: {file_info['overlap_percentage']:.1f}%\")\n",
    "    print(f\"  Nearby satellites: {crossing['num_nearby_satellites']}\")\n",
    "    \n",
    "    if crossing['closest_satellite']:\n",
    "        print(f\"  Closest satellite: {crossing['closest_satellite']}\")\n",
    "        print(f\"  Minimum separation: {crossing['min_separation_deg']:.3f}¬∞\")\n",
    "    \n",
    "    # Add to summary list\n",
    "    summary_dict = {\n",
    "        'filename': file_info['filename'],\n",
    "        'ra_deg': file_info['ra_deg'],\n",
    "        'dec_deg': file_info['dec_deg'],\n",
    "        'tstart_mjd': file_info['tstart_mjd'],\n",
    "        'freq_overlap_pct': file_info['overlap_percentage'],\n",
    "        'num_nearby_satellites': crossing['num_nearby_satellites'],\n",
    "        'closest_satellite': crossing['closest_satellite'],\n",
    "        'min_separation_deg': crossing['min_separation_deg']\n",
    "    }\n",
    "    summary_list.append(summary_dict)\n",
    "\n",
    "# Convert to DataFrame for easy manipulation\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "print(f\"\\nSummary DataFrame:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Satellite Crossing Analysis Results', fontsize=16)\n",
    "\n",
    "# 1. Sky positions of observations\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['red' if n > 0 else 'blue' for n in summary_df['num_nearby_satellites']]\n",
    "scatter = ax1.scatter(summary_df['ra_deg'], summary_df['dec_deg'], \n",
    "                     c=colors, s=100, alpha=0.7)\n",
    "ax1.set_xlabel('Right Ascension (degrees)')\n",
    "ax1.set_ylabel('Declination (degrees)')\n",
    "ax1.set_title('Observation Positions\\n(Red: with satellites, Blue: no satellites)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in summary_df.iterrows():\n",
    "    ax1.annotate(f'F{i+1}', (row['ra_deg'], row['dec_deg']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 2. Number of nearby satellites per file\n",
    "ax2 = axes[0, 1]\n",
    "bars = ax2.bar(range(len(summary_df)), summary_df['num_nearby_satellites'], \n",
    "               color=['red' if n > 0 else 'gray' for n in summary_df['num_nearby_satellites']])\n",
    "ax2.set_xlabel('File Index')\n",
    "ax2.set_ylabel('Number of Nearby Satellites')\n",
    "ax2.set_title('Satellites Within 1¬∞ of Each Observation')\n",
    "ax2.set_xticks(range(len(summary_df)))\n",
    "ax2.set_xticklabels([f'F{i+1}' for i in range(len(summary_df))])\n",
    "\n",
    "# 3. Minimum separation distances (for files with satellites)\n",
    "ax3 = axes[1, 0]\n",
    "files_with_sats = summary_df[summary_df['num_nearby_satellites'] > 0]\n",
    "if len(files_with_sats) > 0:\n",
    "    bars = ax3.bar(range(len(files_with_sats)), files_with_sats['min_separation_deg'], \n",
    "                   color='orange', alpha=0.7)\n",
    "    ax3.set_xlabel('File Index (with satellites)')\n",
    "    ax3.set_ylabel('Minimum Separation (degrees)')\n",
    "    ax3.set_title('Closest Satellite Distance')\n",
    "    ax3.set_xticks(range(len(files_with_sats)))\n",
    "    file_indices = [i+1 for i, row in summary_df.iterrows() if row['num_nearby_satellites'] > 0]\n",
    "    ax3.set_xticklabels([f'F{i}' for i in file_indices])\n",
    "    ax3.axhline(y=max_separation_deg, color='red', linestyle='--', \n",
    "                label=f'{max_separation_deg}¬∞ threshold')\n",
    "    ax3.legend()\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No satellites found\\nwithin 1¬∞ of any observation', \n",
    "             transform=ax3.transAxes, ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Closest Satellite Distance')\n",
    "\n",
    "# 4. Frequency overlap distribution\n",
    "ax4 = axes[1, 1]\n",
    "bins = np.linspace(summary_df['freq_overlap_pct'].min(), \n",
    "                   summary_df['freq_overlap_pct'].max(), 10)\n",
    "ax4.hist(summary_df['freq_overlap_pct'], bins=bins, alpha=0.7, color='green', edgecolor='black')\n",
    "ax4.set_xlabel('Frequency Overlap with 10.6-11.2 GHz (%)')\n",
    "ax4.set_ylabel('Number of Files')\n",
    "ax4.set_title('Target Frequency Range Overlap')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results to file for future reference\n",
    "output_file = Path.cwd() / 'satellite_crossing_results.csv'\n",
    "summary_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b30861",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "\n",
    "This notebook demonstrates how to correlate RFI pipeline observations with Starlink satellite positions to identify potential interference sources. \n",
    "\n",
    "### Key Findings:\n",
    "- We analyzed observations with significant overlap in the 10.6-11.2 GHz frequency range\n",
    "- For each observation, we calculated satellite positions and identified those within 1¬∞ of the beam center\n",
    "- The analysis provides satellite names, angular separations, and detailed position information\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Time Resolution**: Consider the full observation duration, not just the start time\n",
    "2. **Beam Pattern**: Account for the actual telescope beam shape and sensitivity pattern\n",
    "3. **Satellite Movement**: Track satellite motion during the observation period\n",
    "4. **Historical TLEs**: Use TLE data that matches the observation epoch for better accuracy\n",
    "5. **Other Satellite Constellations**: Extend analysis to include OneWeb, Amazon Kuiper, etc.\n",
    "6. **Correlation with RFI Detections**: Compare satellite positions with actual RFI hit locations\n",
    "\n",
    "### Data Products:\n",
    "- `satellite_crossings`: Complete analysis results for each file\n",
    "- `summary_df`: DataFrame with key metrics for each observation\n",
    "- `satellite_crossing_results.csv`: Saved summary for future analysis\n",
    "\n",
    "This framework can be extended to process larger datasets and provide systematic satellite interference monitoring for radio astronomy observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pickles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
